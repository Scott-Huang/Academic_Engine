{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\r\n",
    "import spacy\r\n",
    "import logging\r\n",
    "import gensim\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "import multiprocessing\r\n",
    "\r\n",
    "from time import time\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from gensim.models.word2vec import LineSentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "SENTENCES_FILE = './sentences.txt'\r\n",
    "DATA_FILE = './arxiv-metadata-oai-snapshot.json'\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\r\n",
    "nlp.enable_pipe(\"senter\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "MIN_SENT_LEN = 5\r\n",
    "def cleaning(sent, l):\r\n",
    "    txt = [token.lemma_.lower() \r\n",
    "                for token in sent if not token.is_stop and token.is_alpha]\r\n",
    "    if len(txt) >= MIN_SENT_LEN:\r\n",
    "        # tag phrases as: 'data cleaning' -> 'data_cleaning'\r\n",
    "        terms = find_ngram(txt)\r\n",
    "        out = ' '.join(txt) + '\\n'\r\n",
    "        for term in terms:\r\n",
    "            out = out.replace(term, term.replace(' ', '_'))\r\n",
    "        l.append(out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "in_file = open(DATA_FILE, 'r')\r\n",
    "out_file = open(SENTENCES_FILE, 'w+', buffering=10000000)\r\n",
    "count = 0\r\n",
    "start = 1230001\r\n",
    "\r\n",
    "t = time()\r\n",
    "for line in in_file.readlines():\r\n",
    "    if count < start:\r\n",
    "        count += 1\r\n",
    "        if count == start:\r\n",
    "            print('start to process')\r\n",
    "        continue\r\n",
    "\r\n",
    "    paper = json.loads(line)\r\n",
    "    abstract = paper['abstract'].strip().replace('-\\n', '').replace('\\n', ' ')\r\n",
    "\r\n",
    "    doc = nlp(abstract)\r\n",
    "    txt = []\r\n",
    "    for sent in doc.sents:\r\n",
    "        cleaning(sent, txt)\r\n",
    "    if len(txt) > 0:\r\n",
    "        out_file.writelines(txt)\r\n",
    "    \r\n",
    "    count += 1\r\n",
    "    if count % 50000 == 0:\r\n",
    "        print('finished %d in %f min' % (count, round((time() - t) / 60, 3)))\r\n",
    "\r\n",
    "in_file.close()\r\n",
    "out_file.close()\r\n",
    "print('finished in {} min'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start to process\n",
      "finished 1250000 in 6.076000 min\n",
      "finished 1300000 in 20.210000 min\n",
      "finished 1350000 in 34.547000 min\n",
      "finished 1400000 in 48.820000 min\n",
      "finished 1450000 in 63.108000 min\n",
      "finished 1500000 in 77.275000 min\n",
      "finished 1550000 in 92.255000 min\n",
      "finished 1600000 in 103.979000 min\n",
      "finished 1650000 in 115.073000 min\n",
      "finished 1700000 in 125.242000 min\n",
      "finished 1750000 in 135.275000 min\n",
      "finished 1800000 in 145.037000 min\n",
      "finished 1850000 in 154.648000 min\n",
      "finished in 164.54 min\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "file = open(SENTENCES_FILE)\r\n",
    "sentences = LineSentence(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "cores = multiprocessing.cpu_count()\r\n",
    "w2v_model = Word2Vec(min_count=1,vector_size=128,negative=5,workers=cores-2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def print_time(t):\r\n",
    "    print('finished in {} mins'.format(round((time() - t) / 60, 3)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "t = time()\r\n",
    "# construct vocab set\r\n",
    "w2v_model.build_vocab(sentences, progress_per=1500000)\r\n",
    "\r\n",
    "print_time(t)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished in 0.924 mins\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "t = time()\r\n",
    "\r\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, \r\n",
    "                epochs=60, report_delay=60)\r\n",
    "\r\n",
    "print_time(t)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished training in 108.43 mins\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "w2v_model.save('word2vec_model/word2vec.model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#w2v_model.wv.similarity('optimal', 'optimization')\r\n",
    "w2v_model.wv.most_similar('happy')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('disgusted', 0.47324278950691223),\n",
       " ('disgust', 0.47088298201560974),\n",
       " ('unhappy', 0.46743878722190857),\n",
       " ('sadness', 0.4506657123565674),\n",
       " ('event_source_code', 0.4493028521537781),\n",
       " ('bryn', 0.4440755546092987),\n",
       " ('dissatisfied', 0.4411503076553345),\n",
       " ('brainy', 0.4173509478569031),\n",
       " ('angry', 0.41401079297065735),\n",
       " ('emotion', 0.4129962623119354)]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "KEYWORD_FILE = 'computer_science_keywords.csv'\r\n",
    "KEYWORDS = pd.read_csv(KEYWORD_FILE, header=None, index_col=False)\r\n",
    "KEYWORDS.drop(columns=[2, 3, 4, 5], inplace=True)\r\n",
    "KEYWORDS.rename(columns={0: 'terms', 1: 'num'}, inplace=True)\r\n",
    "KEYWORDS['terms'] = KEYWORDS['terms'].astype(str)\r\n",
    "\r\n",
    "keyword_freq = {}\r\n",
    "for term, num in zip(KEYWORDS['terms'], KEYWORDS['num']):\r\n",
    "    if num.isdigit():\r\n",
    "        keyword_freq[term] = int(num)\r\n",
    "    else:\r\n",
    "        keyword_freq[term] = 0\r\n",
    "\r\n",
    "del KEYWORDS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def ngram(input_list, ngram_num):\r\n",
    "    '''Create a list of ngrams given a list of words.'''\r\n",
    "    ngram_list = []\r\n",
    "    if len(input_list) >= ngram_num:\r\n",
    "        for tmp in zip(*[input_list[i:] for i in range(ngram_num)]):\r\n",
    "            ngram = ''\r\n",
    "            for word in tmp:\r\n",
    "                ngram += word + ' '\r\n",
    "            ngram_list.append(ngram[:-1])\r\n",
    "    return ngram_list\r\n",
    "\r\n",
    "def find_ngram(nouns):\r\n",
    "    terms = set()\r\n",
    "    for i in reversed(range(2,5)):\r\n",
    "        ngram_list = ngram(nouns, i)\r\n",
    "        for term in ngram_list:\r\n",
    "            if term in keyword_freq:\r\n",
    "                terms.add(term)\r\n",
    "    terms = list(terms)\r\n",
    "    terms.sort(key=lambda ele:len(ele), reverse=True)\r\n",
    "    return terms"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}