{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# for debugging the environment if got import error\r\n",
    "sys.path.insert(0, '')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, sys\r\n",
    "import spacy\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "import whoosh\r\n",
    "import gensim\r\n",
    "\r\n",
    "from collections import defaultdict\r\n",
    "from whoosh import index, writing\r\n",
    "from whoosh.qparser import QueryParser\r\n",
    "from whoosh.fields import Schema, TEXT, STORED\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from queue import PriorityQueue\r\n",
    "from heapq import nlargest\r\n",
    "from time import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load keywords file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "KEYWORD_FILE = 'computer_science_keywords.csv'\r\n",
    "KEYWORDS = pd.read_csv(KEYWORD_FILE, header=None, index_col=False)\r\n",
    "KEYWORDS.drop(columns=[2, 3, 4, 5], inplace=True)\r\n",
    "KEYWORDS.rename(columns={0: 'terms', 1: 'num'}, inplace=True)\r\n",
    "KEYWORDS['terms'] = KEYWORDS['terms'].astype(str)\r\n",
    "KEYWORDS.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               terms   num\n",
       "0   machine learning  5933\n",
       "1  genetic algorithm  5264\n",
       "2     classification  4532\n",
       "3      deep learning  4371\n",
       "4        data mining  4369"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>5933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>genetic algorithm</td>\n",
       "      <td>5264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classification</td>\n",
       "      <td>4532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deep learning</td>\n",
       "      <td>4371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data mining</td>\n",
       "      <td>4369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "keyword_freq = {}\r\n",
    "for term, num in zip(KEYWORDS['terms'], KEYWORDS['num']):\r\n",
    "    if num.isdigit():\r\n",
    "        keyword_freq[term] = int(num)\r\n",
    "    else:\r\n",
    "        keyword_freq[term] = 0\r\n",
    "\r\n",
    "del KEYWORDS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieve concept terms from abstract"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# light model for preprocessing the text\r\n",
    "# nlp = spacy.load('en_core_web_sm')\r\n",
    "# model that can be also used for word and doc embedding\r\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "def ngram(input_list, ngram_num):\r\n",
    "    '''Create a list of ngrams given a list of words.'''\r\n",
    "    ngram_list = []\r\n",
    "    if len(input_list) < ngram_num:\r\n",
    "        ngram_list.append(input_list)\r\n",
    "    else:\r\n",
    "        for tmp in zip(*[input_list[i:] for i in range(ngram_num)]):\r\n",
    "            ngram = ''\r\n",
    "            for word in tmp:\r\n",
    "                ngram += word + ' '\r\n",
    "            ngram_list.append(ngram[:-1])\r\n",
    "    return ngram_list\r\n",
    "\r\n",
    "def find_ngram(nouns):\r\n",
    "    '''Find any possible keyword in the input list. Longer keywords have higher priority.'''\r\n",
    "    for i in reversed(range(1,len(nouns)+1)):\r\n",
    "        ngram_list = ngram(nouns, i)\r\n",
    "        for term in ngram_list:\r\n",
    "            if term in keyword_freq:\r\n",
    "                return term\r\n",
    "    return None\r\n",
    "\r\n",
    "def find_concept_terms(abstract):\r\n",
    "    '''Return a str of dict containing keywords and their position from an abstract.'''\r\n",
    "    doc = nlp(abstract)\r\n",
    "    terms = defaultdict(lambda: [])\r\n",
    "    nouns = []\r\n",
    "    for i,token in enumerate(doc):\r\n",
    "        if token.pos_ == 'NOUN':\r\n",
    "            nouns.append(token.text.lower())\r\n",
    "        elif nouns:\r\n",
    "            term = find_ngram(nouns)\r\n",
    "            if term:\r\n",
    "                terms[term].append(i)\r\n",
    "            nouns = []\r\n",
    "    if nouns:\r\n",
    "        term = find_ngram(nouns)\r\n",
    "        if term:\r\n",
    "            terms[term].append(i)\r\n",
    "    return repr(dict(terms))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Design search engine and read documents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "schema = Schema(abstract=TEXT(),tags=STORED())\r\n",
    "\r\n",
    "INDEX_FOLDER = 'index_data'\r\n",
    "if not os.path.exists(INDEX_FOLDER):\r\n",
    "    os.mkdir(INDEX_FOLDER)\r\n",
    "    paper_index = index.create_in(INDEX_FOLDER, schema)\r\n",
    "else:\r\n",
    "    paper_index = index.open_dir(INDEX_FOLDER)\r\n",
    "\r\n",
    "def clear_index(index):\r\n",
    "    writer = index.writer()\r\n",
    "    writer.commit(mergetype=writing.CLEAR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "DATA_FILE = 'arxiv-metadata-oai-snapshot.json'\r\n",
    "start = 900001\r\n",
    "num_paper = 1000000\r\n",
    "file = open(DATA_FILE, 'r')\r\n",
    "writer = paper_index.writer()\r\n",
    "count = 0\r\n",
    "\r\n",
    "for line in file.readlines():\r\n",
    "    if count > num_paper:\r\n",
    "        break\r\n",
    "    if count < start:\r\n",
    "        if count == start - 1:\r\n",
    "            print('start to process')\r\n",
    "        count += 1\r\n",
    "        continue\r\n",
    "\r\n",
    "    paper = json.loads(line)\r\n",
    "    abstract = paper['abstract'].strip().lower().replace('-\\n', '').replace('\\n', ' ')\r\n",
    "    tags = find_concept_terms(abstract)\r\n",
    "\r\n",
    "    writer.add_document(abstract=abstract,tags = tags)\r\n",
    "\r\n",
    "    count += 1\r\n",
    "    if count % 10000 == 0:\r\n",
    "        print('Complete %d files' % count)\r\n",
    "\r\n",
    "print('start to commit...')\r\n",
    "writer.commit()\r\n",
    "print('finish commiting')\r\n",
    "file.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start to process\n",
      "Complete 910000 files\n",
      "Complete 920000 files\n",
      "Complete 930000 files\n",
      "Complete 940000 files\n",
      "Complete 950000 files\n",
      "Complete 960000 files\n",
      "Complete 970000 files\n",
      "Complete 980000 files\n",
      "Complete 990000 files\n",
      "Complete 1000000 files\n",
      "start to commit...\n",
      "finish commiting\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rank concepts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "entity243 is good and equivalent to entity3825\n",
    "\n",
    "apple data order\n",
    "What is entity243(apple) equivalent to?\n",
    "How people think of apple\n",
    "entity3825"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ranking formula:\n",
    "$$\\text{Score}_q(t) = (1-\\alpha) P_q(t|D) + \\alpha P(t|\\bar{S}),$$\n",
    "where \n",
    "$$P_q(t|D) = \\sum_{d\\in D} P_q(t|d) \\\\ P(t|d) = \\frac{\\log{\\text{bm25}_d(q)} * \\text{freq}_d(t)}{\\text{freq}_D(t)^{k1} \\text{sim}(t,q)^{k2}},$$\n",
    "and\n",
    "$$P(t|\\bar{S})\\equiv 0.05+\\parallel\\text{sim}(t,S) \\parallel_{10}^{-2} = 0.05+\\sqrt[5]{\\sum_{s\\in S} \\text{sim}(t,s)^{-10}}.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def relu(x):\r\n",
    "    return x * (x > 0)\r\n",
    "X = np.arange(16) * 10\r\n",
    "Y = np.array([3.3,1.98,1.38,0.99,0.86,0.81,0.77,0.7,0.52,0.42,0.4,0.37,0.35,0.33,0.3,0.25])\r\n",
    "Y = (Y - 0.2)\r\n",
    "z = np.polyfit(X, Y, 5)\r\n",
    "p = np.poly1d(z)\r\n",
    "\r\n",
    "# the probability between span and relevance\r\n",
    "def span_prox(s):\r\n",
    "    return p(s) * (s < 150) + 0.2\r\n",
    "plt.plot(span_prox(np.arange(200)))\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def prob(word_position, key_position):\r\n",
    "    '''Return the total probability of the word and the keyword being related.'''\r\n",
    "    p = 0\r\n",
    "    for key_pos in key_position:\r\n",
    "        p += span_prox(np.abs(word_position - key_pos)).sum()\r\n",
    "    return p"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "qp = QueryParser(\"abstract\", schema=schema)\r\n",
    "def search(s, word):\r\n",
    "    '''Return a generator of searched results and word occurance'''\r\n",
    "    q = qp.parse(word)\r\n",
    "    \r\n",
    "    matcher = q.matcher(s)\r\n",
    "    # brutal force actually faster than priority queue\r\n",
    "    positions = {}\r\n",
    "    while matcher.is_active():\r\n",
    "        start = tuple([span.start for span in matcher.spans()])\r\n",
    "        positions[start] = matcher.score()\r\n",
    "        matcher.next()\r\n",
    "    positions = sorted(positions.items(), key=lambda kv: kv[1], reverse=True)\r\n",
    "    positions = [np.array(elem[0]) for elem in positions]\r\n",
    "    \r\n",
    "    results = s.search(q, limit=5000)\r\n",
    "    return zip(results,positions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def soft_freq(prob, freq):\r\n",
    "    return prob / np.sqrt(freq)\r\n",
    "def hard_freq(prob, freq):\r\n",
    "    return prob / freq ** 1.05\r\n",
    "def soft_dif_word(value, target, concept):\r\n",
    "    return value / (compare_similarity(target, concept) + 1)\r\n",
    "def hard_dif_word(value, target, concept):\r\n",
    "    return value / (compare_similarity(target, concept) + 0.2) ** 2\r\n",
    "def soft_sim_word(value, target, concept):\r\n",
    "    return value * (compare_similarity(target, concept) + 1)\r\n",
    "def hard_sim_word(value, target, concept):\r\n",
    "    return value * (compare_similarity(target, concept) + 0.2) ** 2\r\n",
    "freq_f = {'soft freq': soft_freq, 'hard freq': hard_freq}\r\n",
    "sim_f = {'soft dif': soft_dif_word, 'hard dif': hard_dif_word, 'soft sim': soft_sim_word, 'hard sim': hard_sim_word}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "w2v_model = Word2Vec.load('word2vec_model/word2vec.model')\r\n",
    "def lemmatize(concept):\r\n",
    "    doc = nlp(concept)\r\n",
    "    txt = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\r\n",
    "    return ('_'.join(txt)).replace('datum', 'data')\r\n",
    "def compare_similarity(word1, word2):\r\n",
    "    try:\r\n",
    "        return abs(w2v_model.wv.similarity(word1, word2))\r\n",
    "    except:\r\n",
    "        out = nlp(word1.replace('_', ' ')).similarity(nlp(word2.replace('_', ' ')))\r\n",
    "        if not out:\r\n",
    "            out = 0.05\r\n",
    "        return out\r\n",
    "def normalize(v):\r\n",
    "    norm = np.linalg.norm(v)\r\n",
    "    if norm == 0: \r\n",
    "        return v\r\n",
    "    return v / norm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def diversify(concepts, k, alpha):\r\n",
    "    out = []\r\n",
    "    concept_len = len(concepts)\r\n",
    "    if concept_len == 0:\r\n",
    "        return []\r\n",
    "\r\n",
    "    sim_table = np.zeros((concept_len, k))\r\n",
    "    concept_value = normalize(np.array(list(concepts.values())))\r\n",
    "    concept_id = list(concepts.keys())\r\n",
    "    \r\n",
    "    for i in range(k):\r\n",
    "        norm = np.linalg.norm(sim_table, ord=10, axis=1) ** 2 + 0.05\r\n",
    "        norm = normalize(1/norm)\r\n",
    "        temp = concept_value * (1-alpha) +  norm * alpha\r\n",
    "        argmax = temp.argmax()\r\n",
    "\r\n",
    "        concept = concept_id.pop(argmax)\r\n",
    "        out.append(concept)\r\n",
    "        sim_table = np.delete(sim_table,argmax,axis=0)\r\n",
    "        concept_value = np.delete(concept_value,argmax,axis=0)\r\n",
    "\r\n",
    "        min_sim = 0\r\n",
    "        for j in range(concept_len - i - 1):\r\n",
    "            sim = compare_similarity(concept, concept_id[j])\r\n",
    "            sim_table[j,i] = sim\r\n",
    "    \r\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# load file\r\n",
    "keyword_actual_freq = np.load('keyword_freq.npy', allow_pickle='TRUE').item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "qp = QueryParser(\"abstract\", schema=schema)\r\n",
    "def rank(s, word, freq_function='auto', word_function='', k=10, diverse=True, alpha=0.5):\r\n",
    "    '''Rank all concept words.'''\r\n",
    "    \r\n",
    "    # sum their probability\r\n",
    "    r = defaultdict(lambda: 0)\r\n",
    "\r\n",
    "    '''\r\n",
    "    # take account the factor of distance between words\r\n",
    "    for hit,position in search(s, word):\r\n",
    "        tags = eval(hit['tags'])\r\n",
    "        for keyword, key_position in tags.items():\r\n",
    "            r[keyword] += prob(position, key_position) # evaluate relevance by their spans\r\n",
    "    '''\r\n",
    "\r\n",
    "    q = qp.parse(word)\r\n",
    "    results = s.search(q, limit=20000)\r\n",
    "    for hit in results:\r\n",
    "        tags = eval(hit['tags'])\r\n",
    "        score = np.log(hit.score)\r\n",
    "        for keyword, key_position in tags.items():\r\n",
    "            r[keyword] += score * len(key_position)\r\n",
    "\r\n",
    "    if freq_function == 'auto':\r\n",
    "        count = len(results)\r\n",
    "        if count > 1000:\r\n",
    "            freq_function = 'hard freq'\r\n",
    "        elif count >= 10:\r\n",
    "            freq_function = 'soft freq'\r\n",
    "        else:\r\n",
    "            freq_function = ''\r\n",
    "    \r\n",
    "    # further calculation\r\n",
    "    r = dict(r)\r\n",
    "    r.pop(word, '')\r\n",
    "\r\n",
    "    topwords = defaultdict(int)\r\n",
    "    for concept,probability in nlargest(600, r.items(), key=lambda x: x[1]):\r\n",
    "        value = probability\r\n",
    "        # adjust by freq of concept terms\r\n",
    "        if freq_function in freq_f:\r\n",
    "            value = freq_f[freq_function](value, keyword_actual_freq[concept])\r\n",
    "        # adjust by similarity\r\n",
    "        if word_function in sim_f:\r\n",
    "            value = sim_f[word_function](value, word, concept)\r\n",
    "        topwords[lemmatize(concept)] += value\r\n",
    "    topwords = dict(topwords)\r\n",
    "    topwords.pop('', '')\r\n",
    "    \r\n",
    "    if diverse and 1 > alpha and 0 < alpha:\r\n",
    "        return diversify(topwords, k, alpha)\r\n",
    "    return nlargest(k, topwords.items(), key=lambda x: x[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def show_time_s(t):\r\n",
    "    print('finished in {} seconds'.format(round((time() - t), 3)))\r\n",
    "    return time()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "with paper_index.searcher() as s:\r\n",
    "    #word = 'panopticon' # 1\r\n",
    "    #word = 'crnn' # 12\r\n",
    "    #word = 'agriculture' # 175\r\n",
    "    #word = 'soil' # 436\r\n",
    "    #word = 'rl' # 677\r\n",
    "    #word = 'compression' # 4796\r\n",
    "    word = 'electromagnetic'\r\n",
    "    print(word)\r\n",
    "    t = time()\r\n",
    "    print(rank(s, word, diverse=False))\r\n",
    "    t = show_time_s(t)\r\n",
    "    print(rank(s, word))\r\n",
    "    t = show_time_s(t)\r\n",
    "    try:\r\n",
    "        print(w2v_model.wv.most_similar(word.replace(' ', '_')))\r\n",
    "    except:\r\n",
    "        print('%s not in the model' % word)\r\n",
    "    t = show_time_s(t)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "electromagnetic\n",
      "[('maxwell_equation', 0.4749767281113908), ('cloak', 0.4300841292782278), ('electromagnetism', 0.39382796215394), ('metamaterial', 0.3229241860425833), ('em', 0.28350851072408473), ('permittivity', 0.2276361626340059), ('quantum_electrodynamic', 0.21773908168262363), ('form_factor', 0.21204209504644117), ('cascade', 0.1964064364972067), ('wave', 0.1957129560070033)]\n",
      "finished in 4.367 seconds\n",
      "['maxwell_equation', 'cloak', 'electromagnetism', 'metamaterial', 'em', 'form_factor', 'cascade', 'permittivity', 'wave', 'quantum_electrodynamic']\n",
      "finished in 8.105 seconds\n",
      "[('electromagnetic_field', 0.6270819306373596), ('electromagnetic_radiation', 0.6187474727630615), ('electromagnetic_wave', 0.5709630250930786), ('electric', 0.5479849576950073), ('em', 0.5152329206466675), ('electrodynamic', 0.5121166706085205), ('radiation', 0.49558883905410767), ('magnetic_dipole', 0.48694825172424316), ('gravitational_wave', 0.47556033730506897), ('gravitational', 0.47068583965301514)]\n",
      "finished in 0.061 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "# keyword freq\r\n",
    "keyword_actual_freq = defaultdict(lambda:0)\r\n",
    "with paper_index.searcher() as s:\r\n",
    "    all_doc = s.documents()\r\n",
    "    for doc in all_doc:\r\n",
    "        tags = eval(doc['tags'])\r\n",
    "        for tag, occurence in tags.items():\r\n",
    "            keyword_actual_freq[tag] += len(occurence)\r\n",
    "# save to file\r\n",
    "keyword_actual_freq = dict(keyword_actual_freq)\r\n",
    "np.save('keyword_freq.npy', keyword_actual_freq)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "a2b4dac113a1be4bc15d81ea2b8c8f9e10349855d4a4b1e57052da3bc1163078"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}